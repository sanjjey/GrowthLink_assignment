{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPnwIGihlga2QftYU5C8lX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjjey/GrowthLink_assignment/blob/main/ML_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJuh0MvsFyao",
        "outputId": "053b922a-74a7-4f30-9309-a2d59bccc1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nlpaug in /usr/local/lib/python3.11/dist-packages (1.1.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (2.32.3)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug) (3.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.22.0->nlpaug) (2025.1.31)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.13.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nlpaug nltk pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"uciml/sms-spam-collection-dataset\")\n",
        "path=path+\"/spam.csv\"\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfO1PRFMF03W",
        "outputId": "3ab374a9-5c30-4e5b-ccc1-1b2ac32a1a58"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/sms-spam-collection-dataset/spam.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nltk\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Download necessary resources for nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "# Load the CSV file\n",
        "df = pd.read_csv(path,encoding=\"utf-8\",encoding_errors=\"ignore\")\n",
        "#Separating ham and spam\n",
        "\n",
        "ham_df = df[df['v1'] == 'ham']\n",
        "spam_df = df[df['v1'] == 'spam']\n",
        "\n",
        "#Sampling the minority to equalize the majority(oversampling)\n",
        "n_ham = len(ham_df)\n",
        "n_spam = len(spam_df)\n",
        "n_to_generate = n_ham - n_spam\n",
        "\n",
        "# Initialize NLP augmentation using wordnet\n",
        "aug = naw.SynonymAug(aug_src='wordnet')\n",
        "\n",
        "# Generate synthetic spam samples\n",
        "augmented_texts = []\n",
        "original_texts = spam_df['v2'].tolist()\n",
        "\n",
        "i = 0\n",
        "while len(augmented_texts) < n_to_generate:\n",
        "    aug_text = aug.augment(original_texts[i % len(original_texts)])\n",
        "    augmented_texts.append(aug_text)\n",
        "    i += 1\n",
        "\n",
        "# Create a DataFrame for the synthetic spam data\n",
        "augmented_spam_df = pd.DataFrame({'v1': ['spam'] * len(augmented_texts), 'v2': augmented_texts})\n",
        "\n",
        "# Combine with the original data\n",
        "balanced_df = pd.concat([ham_df, spam_df, augmented_spam_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Save the new balanced dataset\n",
        "balanced_df.to_csv('balanced_data.csv', index=False)\n",
        "\n",
        "print(f\"Original spam: {n_spam}, Generated spam: {len(augmented_spam_df)}, Total: {len(balanced_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxY9pvAGF6gC",
        "outputId": "20f4a241-8b67-4426-9666-9d3aef6de11d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original spam: 747, Generated spam: 4078, Total: 9650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=pd.read_csv(\"balanced_data.csv\")#dataset loaded"
      ],
      "metadata": {
        "id": "2tL5nwDfGLY_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['v1'].value_counts()#checking if the data sampling worked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "qf4kxvk6GwUP",
        "outputId": "88128be8-45fa-4b11-8d02-a66705eb01de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "v1\n",
              "spam    4825\n",
              "ham     4825\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>v1</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>spam</th>\n",
              "      <td>4825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ham</th>\n",
              "      <td>4825</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import xgboost as xgb\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "df=dataset\n",
        "\n",
        "# Preprocess text data: Tokenize and vectorize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize the text\n",
        "df['tokens'] = df['v2'].apply(lambda x: word_tokenize(x.lower()))\n",
        "\n",
        "# Label encoding for target variable\n",
        "label_encoder = LabelEncoder()\n",
        "df['v1'] = label_encoder.fit_transform(df['v1'])\n",
        "\n",
        "# Split into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['tokens'], df['v1'], test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization for XGBoost\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train.apply(lambda x: ' '.join(x)))\n",
        "X_test_tfidf = vectorizer.transform(X_test.apply(lambda x: ' '.join(x)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPaW7d_NG0eB",
        "outputId": "944b3714-88c1-48ab-f2f7-32318d12f737"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
        "xgb_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions\n",
        "xgb_preds = xgb_model.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate XGBoost\n",
        "xgb_accuracy = accuracy_score(y_test, xgb_preds)\n",
        "print(f'XGBoost Accuracy: {xgb_accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCtUzzv-JyFi",
        "outputId": "365c8984-3bfd-4cf0-cd36-2682750e6623"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Accuracy: 0.9865\n"
          ]
        }
      ]
    }
  ]
}